{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles saved to news_articles.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "class NewsScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"AsmaNewsScraper/1.0 (+mailto:asma@example.com)\"\n",
    "        }\n",
    "\n",
    "    def fetch_articles(self, url, title_selector, content_selector=None, max_articles=5):\n",
    "        \"\"\"\n",
    "        Fetches articles from the specified website.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL to scrape.\n",
    "            title_selector (dict): CSS selector for article titles and links.\n",
    "            content_selector (dict): CSS selector for article content.\n",
    "            max_articles (int): Maximum number of articles to fetch.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of articles with title, link, and content.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract titles and links\n",
    "            articles = soup.find_all(**title_selector)[:max_articles]\n",
    "            article_list = []\n",
    "\n",
    "            for article in articles:\n",
    "                title = article.get_text(strip=True)\n",
    "                link = article['href']\n",
    "                content = self.fetch_article_content(link, content_selector)\n",
    "                article_list.append({'title': title, 'link': link, 'content': content})\n",
    "                time.sleep(2)  # Rate-limiting to avoid overwhelming the server\n",
    "\n",
    "            return article_list\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching articles from {url}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_article_content(self, url, content_selector):\n",
    "        \"\"\"\n",
    "        Fetches the content of an article.\n",
    "\n",
    "        Args:\n",
    "            url (str): The article URL.\n",
    "            content_selector (dict): CSS selector for article content.\n",
    "\n",
    "        Returns:\n",
    "            str: The article content or an error message.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            if content_selector:\n",
    "                content_div = soup.find(**content_selector)\n",
    "                if content_div:\n",
    "                    return content_div.get_text(strip=True)\n",
    "            return \"Content not found or format not supported.\"\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching content from {url}: {e}\")\n",
    "            return \"Error fetching content.\"\n",
    "\n",
    "    def save_to_json(self, articles, filename):\n",
    "        \"\"\"\n",
    "        Saves articles to a JSON file.\n",
    "\n",
    "        Args:\n",
    "            articles (list): List of articles to save.\n",
    "            filename (str): Name of the JSON file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(articles, f, indent=4)\n",
    "        print(f\"Articles saved to {filename}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = NewsScraper()\n",
    "\n",
    "    # TechCrunch\n",
    "    techcrunch_articles = scraper.fetch_articles(\n",
    "        url=\"https://techcrunch.com/\",\n",
    "        title_selector={\"name\": \"a\", \"class_\": \"loop-card__title-link\"},\n",
    "        content_selector={\"name\": \"p\", \"id\": \"speakable-summary\"}\n",
    "    )\n",
    "\n",
    "    # The Verge\n",
    "    verge_articles = scraper.fetch_articles(\n",
    "        url=\"https://www.theverge.com/tech\",\n",
    "        title_selector={\"name\": \"a\", \"class_\": \"c-entry-box--compact__title\"},\n",
    "        content_selector={\"name\": \"div\", \"class_\": \"duet--article--article-body-component\"}\n",
    "    )\n",
    "\n",
    "    # Combine all articles\n",
    "    all_articles = {\n",
    "        \"techcrunch\": techcrunch_articles,\n",
    "        \"theverge\": verge_articles\n",
    "    }\n",
    "\n",
    "    # Save to JSON\n",
    "    scraper.save_to_json(all_articles, \"news_articles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles saved to news_articles.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "class NewsScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"AsmaNewsScraper/1.0 (+mailto:asma@example.com)\"\n",
    "        }\n",
    "\n",
    "    def fetch_articles(self, url, title_selector, content_selector=None, max_articles=5):\n",
    "        \"\"\"\n",
    "        Fetches articles from the specified website.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL to scrape.\n",
    "            title_selector (dict): CSS selector for article titles and links.\n",
    "            content_selector (dict): CSS selector for article content.\n",
    "            max_articles (int): Maximum number of articles to fetch.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of articles with title, link, and content.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract titles and links\n",
    "            articles = soup.find_all(**title_selector)[:max_articles]\n",
    "            article_list = []\n",
    "\n",
    "            for article in articles:\n",
    "                title = article.get_text(strip=True)\n",
    "                link = article['href']\n",
    "                if not link.startswith('http'):  # Handle relative URLs\n",
    "                    link = 'https://www.theverge.com' + link\n",
    "                content = self.fetch_article_content(link, content_selector)\n",
    "                article_list.append({'title': title, 'link': link, 'content': content})\n",
    "                time.sleep(2)  # Rate-limiting to avoid overwhelming the server\n",
    "\n",
    "            return article_list\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching articles from {url}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_article_content(self, url, content_selector):\n",
    "        \"\"\"\n",
    "        Fetches the content of an article.\n",
    "\n",
    "        Args:\n",
    "            url (str): The article URL.\n",
    "            content_selector (dict): CSS selector for article content.\n",
    "\n",
    "        Returns:\n",
    "            str: The article content or an error message.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            if content_selector:\n",
    "                content_div = soup.find(**content_selector)\n",
    "                if content_div:\n",
    "                    return content_div.get_text(strip=True)\n",
    "            return \"Content not found or format not supported.\"\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching content from {url}: {e}\")\n",
    "            return \"Error fetching content.\"\n",
    "\n",
    "    def save_to_json(self, articles, filename):\n",
    "        \"\"\"\n",
    "        Saves articles to a JSON file.\n",
    "\n",
    "        Args:\n",
    "            articles (list): List of articles to save.\n",
    "            filename (str): Name of the JSON file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(articles, f, indent=4)\n",
    "        print(f\"Articles saved to {filename}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = NewsScraper()\n",
    "\n",
    "    # TechCrunch\n",
    "    techcrunch_articles = scraper.fetch_articles(\n",
    "        url=\"https://techcrunch.com/\",\n",
    "        title_selector={\"name\": \"a\", \"class_\": \"loop-card__title-link\"},\n",
    "        content_selector={\"name\": \"p\", \"id\": \"speakable-summary\"}\n",
    "    )\n",
    "\n",
    "    # The Verge\n",
    "    verge_articles = scraper.fetch_articles(\n",
    "        url=\"https://www.theverge.com/tech\",\n",
    "        title_selector={\"name\": \"a\", \"class_\": \"after:absolute after:inset-0 group-hover:shadow-highlight-franklin dark:group-hover:shadow-highlight-blurple\"},\n",
    "        content_selector={\"name\": \"div\", \"class_\": \"duet--article--article-body-component\"}\n",
    "    )\n",
    "\n",
    "    # Combine all articles\n",
    "    all_articles = {\n",
    "        \"techcrunch\": techcrunch_articles,\n",
    "        \"theverge\": verge_articles\n",
    "    }\n",
    "\n",
    "    # Save to JSON\n",
    "    scraper.save_to_json(all_articles, \"news_articles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles saved to news_articles.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "class NewsScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"AsmaNewsScraper/1.0 (+mailto:asma@example.com)\"\n",
    "        }\n",
    "\n",
    "    def fetch_articles(self, url, title_selector, content_selector=None, max_articles=5):\n",
    "        \"\"\"\n",
    "        Fetches articles from the specified website.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL to scrape.\n",
    "            title_selector (dict): CSS selector for article titles and links.\n",
    "            content_selector (dict): CSS selector for article content.\n",
    "            max_articles (int): Maximum number of articles to fetch.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of articles with title, link, and content.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract titles and links\n",
    "            articles = soup.find_all(**title_selector)[:max_articles]\n",
    "            article_list = []\n",
    "\n",
    "            for article in articles:\n",
    "                title = article.get_text(strip=True)\n",
    "                link = article['href']\n",
    "                if not link.startswith('http'):  # Handle relative URLs\n",
    "                    link = 'https://www.wired.com' + link\n",
    "                content = self.fetch_article_content(link, content_selector)\n",
    "                article_list.append({'title': title, 'link': link, 'content': content})\n",
    "                time.sleep(2)  # Rate-limiting to avoid overwhelming the server\n",
    "\n",
    "            return article_list\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching articles from {url}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_article_content(self, url, content_selector):\n",
    "        \"\"\"\n",
    "        Fetches the content of an article.\n",
    "\n",
    "        Args:\n",
    "            url (str): The article URL.\n",
    "            content_selector (dict): CSS selector for article content.\n",
    "\n",
    "        Returns:\n",
    "            str: The article content or an error message.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            if content_selector:\n",
    "                content_div = soup.find(**content_selector)\n",
    "                if content_div:\n",
    "                    return content_div.get_text(strip=True)\n",
    "            return \"Content not found or format not supported.\"\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching content from {url}: {e}\")\n",
    "            return \"Error fetching content.\"\n",
    "\n",
    "    def save_to_json(self, articles, filename):\n",
    "        \"\"\"\n",
    "        Saves articles to a JSON file.\n",
    "\n",
    "        Args:\n",
    "            articles (list): List of articles to save.\n",
    "            filename (str): Name of the JSON file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(articles, f, indent=4)\n",
    "        print(f\"Articles saved to {filename}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = NewsScraper()\n",
    "\n",
    "    # TechCrunch\n",
    "    techcrunch_articles = scraper.fetch_articles(\n",
    "        url=\"https://techcrunch.com/\",\n",
    "        title_selector={\"name\": \"a\", \"class_\": \"loop-card__title-link\"},\n",
    "        content_selector={\"name\": \"p\", \"id\": \"speakable-summary\"}\n",
    "    )\n",
    "\n",
    "    # Wired\n",
    "    wired_articles = scraper.fetch_articles(\n",
    "        url=\"https://www.wired.com/\",\n",
    "        title_selector={\"name\": \"a\", \"attrs\": {\"data-testid\": \"ContentHeaderHed\"}},\n",
    "        content_selector={\"name\": \"p\", \"class_\": \"paywall\"}\n",
    "    )\n",
    "\n",
    "    # Combine all articles\n",
    "    all_articles = {\n",
    "        \"techcrunch\": techcrunch_articles,\n",
    "        \"wired\": wired_articles\n",
    "    }\n",
    "\n",
    "    # Save to JSON\n",
    "    scraper.save_to_json(all_articles, \"news_articles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching articles from https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZ4ZERBU0FtVnVLQUFQAQ?oc=3: 400 Client Error: Bad Request for url: https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZ4ZERBU0FtVnVLQUFQAQ?oc=3&hl=en-CA&gl=CA&ceid=CA:en\n",
      "Articles saved to news_articles.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class NewsScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "    def fetch_articles(self, url, title_selector, content_selector, max_articles=5):\n",
    "        articles_data = []\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Fetch article links\n",
    "            articles = soup.find_all(title_selector[\"name\"], class_=title_selector.get(\"class_\"), limit=max_articles)\n",
    "\n",
    "            for article in articles:\n",
    "                title = article.get_text(strip=True)\n",
    "                link = article[\"href\"]\n",
    "                if not link.startswith(\"http\"):\n",
    "                    link = url + link  # Handle relative URLs\n",
    "\n",
    "                # Fetch article content\n",
    "                try:\n",
    "                    time.sleep(2)  # Wait for 2 seconds to be ethical\n",
    "                    article_response = requests.get(link, headers=self.headers)\n",
    "                    article_response.raise_for_status()\n",
    "                    article_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "                    content = \" \".join(\n",
    "                        [\n",
    "                            p.get_text(strip=True)\n",
    "                            for p in article_soup.find_all(content_selector[\"name\"], class_=content_selector.get(\"class_\"))\n",
    "                        ]\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    content = f\"Error fetching content: {str(e)}\"\n",
    "\n",
    "                articles_data.append({\"title\": title, \"link\": link, \"content\": content})\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching articles from {url}: {str(e)}\")\n",
    "\n",
    "        return articles_data\n",
    "\n",
    "    def save_to_json(self, data, filename):\n",
    "        try:\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "            print(f\"Articles saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to JSON: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = NewsScraper()\n",
    "\n",
    "    # TechCrunch\n",
    "    techcrunch_articles = scraper.fetch_articles(\n",
    "        url=\"https://techcrunch.com/\",\n",
    "        title_selector={\"name\": \"a\", \"class_\": \"loop-card__title-link\"},\n",
    "        content_selector={\"name\": \"p\", \"id\": \"speakable-summary\"}\n",
    "    )\n",
    "\n",
    "    # Google News (replacing Wired)\n",
    "    google_news_articles = scraper.fetch_articles(\n",
    "        url=\"https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZ4ZERBU0FtVnVLQUFQAQ?oc=3\",\n",
    "        title_selector={\"name\": \"a\", \"class_\": \"DY5T1d RZIKme\"},\n",
    "        content_selector={\"name\": \"div\", \"class_\": \"xrnccd\"}\n",
    "    )\n",
    "\n",
    "    # Combine all articles\n",
    "    all_articles = {\n",
    "        \"techcrunch\": techcrunch_articles,\n",
    "        \"google_news\": google_news_articles\n",
    "    }\n",
    "\n",
    "    # Save to JSON\n",
    "    scraper.save_to_json(all_articles, \"news_articles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
